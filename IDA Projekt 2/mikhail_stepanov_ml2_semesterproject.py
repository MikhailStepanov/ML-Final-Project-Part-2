# -*- coding: utf-8 -*-
"""Mikhail_Stepanov_ML2_Semesterproject.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1takSv8w-OlaSB9QL5nzpD-yABwFaVOea
"""

import tensorflow as tf  
 
print(tf.__version__)    

import numpy as np
import matplotlib.pyplot as plt
from tensorflow.keras.layers import Conv2D, Dense, Flatten, Dropout 
from tensorflow.keras.layers import MaxPooling2D
import cv2
from tensorflow import keras
from sklearn.decomposition import PCA
from sklearn.metrics import confusion_matrix
import seaborn as sns

#Data preprocessing
def data_preprocessing(data, grayscale):

    #Distribute it to train and test set
    (x_train, y_train), (x_test, y_test) = data.load_data()

    #Grayscaled representation of the data
    if grayscale:
        x_train_new = []
        x_test_new  = []
        for i in x_train:
            x_train_new.append(cv2.cvtColor(i, cv2.COLOR_BGR2GRAY))
        for i in x_test:
            x_test_new.append(cv2.cvtColor(i, cv2.COLOR_BGR2GRAY))
        
        x_train = np.array(x_train_new)
        x_train = x_train.reshape(x_train.shape[0], x_train.shape[1], x_train.shape[2], 1)

        x_test  = np.array(x_test_new)
        x_test = x_test.reshape(x_test.shape[0], x_test.shape[1], x_test.shape[2], 1)

        print("grayscaled shape x_train", x_train.shape)
        print("grayscaled shape x_test", x_test.shape)
    
    #Data normalization
    x_train = x_train/255
    x_test = x_test/255

    #Categorical representation of the labels
    y_train_categorical = keras.utils.to_categorical(y_train,10)
    y_test_categorical = keras.utils.to_categorical(y_test,10)

    return x_train, x_test, y_train_categorical, y_test_categorical, y_test

#Show firsts 3 smaples of the training dataset
def show_img(x_train, y_train, labels, grayscaled):
    
    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(14,14))

    #reshaped data for grayscaled images
    if grayscaled:
        ax1.imshow(x_train[0].reshape(32,32), cmap="gray")
        ax1.set_title(labels[np.argmax(y_train[0])])
        ax2.imshow(x_train[1].reshape(32,32), cmap="gray")
        ax2.set_title(labels[np.argmax(y_train[1])])
        ax3.imshow(x_train[2].reshape(32,32), cmap="gray")
        ax3.set_title(labels[np.argmax(y_train[2])])

    else:
        ax1.imshow(x_train[0])
        ax1.set_title(labels[np.argmax(y_train[0])])
        ax2.imshow(x_train[1])
        ax2.set_title(labels[np.argmax(y_train[1])])
        ax3.imshow(x_train[2])
        ax3.set_title(labels[np.argmax(y_train[2])])

#Show some missclassified images
def show_missclassified_img(model, x_test, y_test, amount, labels, gray, conf_matrix):

    pred = model.predict(x_test)
    pred = np.argmax(pred, axis=1)

    y_test = y_test.reshape(y_test.shape[0])
    print("predicted", pred[:20])
    print("y_test", y_test[:20])

    compare = pred == y_test

    x_misscl    = x_test[~compare]
    pred_misscl = pred[~compare]

    print(x_misscl.shape)

    for i in range(amount):
        print("Model prediction: "+str(labels[pred_misscl[i]]))
        if gray:
            plt.imshow(x_misscl[i].reshape(32,32), cmap="gray")
            plt.show()
        else:
            plt.imshow(x_misscl[i])
            plt.show()

    #Draw confusion matrix
    if conf_matrix:
        confMat = confusion_matrix(y_test, pred)
        plt.figure(figsize=(12,12))
        sns.heatmap(confMat, cbar=False, xticklabels=labels, yticklabels=labels, fmt="d", annot=True, cmap=plt.cm.Blues)
        plt.xlabel("Predicted")
        plt.ylabel("Actual")
        plt.show()

    return compare

#Show some images, that were missclassified by the FCNN, but correctely classified 
#by CNN model
def compareResults(maskCNN, maskFCNN, y_test, labels, amount):

    maskCompare = np.logical_and(maskCNN, np.logical_not(maskFCNN))
    x_misscl    = x_test[maskCompare]
    y_test      = y_test.reshape(y_test.shape[0])
    label       = y_test[maskCompare]

    for i in range(amount):
        print("Correct label: "+str(labels[label[i]]))
        plt.imshow(x_misscl[i])
        plt.show()

#Check fraction of initial variance of the data, after applying PCA
def check_var_pca(data, remVar, step, start):

    #Flatten data
    new_data = np.reshape(data, (data.shape[0], np.prod(data.shape[1:4])))

    var     = 0
    counter = start

    while var < remVar:
        pca_d = PCA(n_components=counter)
        pca_d.fit(new_data)
        var = sum(pca_d.explained_variance_ratio_)        
        print("# of principal components:", counter, " varinace: ", var)
        counter += step

    return counter

#Use PCA for the dimensionality reduction of the data
def myPca(data, test_data, nComponents):

    new_data = np.reshape(data, (data.shape[0], np.prod(data.shape[1:4])))
    new_test = np.reshape(test_data, (test_data.shape[0], np.prod(test_data.shape[1:4])))

    pca_d = PCA(n_components=nComponents)
    pca_d.fit(new_data)

    new_d    = pca_d.transform(new_data)
    new_test = pca_d.transform(new_test)

    return new_d, new_test

#Plot accuracies and losses on the validation and train dataset dependent on the
#epochs of the training 
def drawAcc(his): 
  
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(17,7))

    ax1.plot(his.history['accuracy'])
    ax1.plot(his.history['val_accuracy'])
    ax1.set_title('model accuracy')
    ax1.set_ylabel('accuracy')
    ax1.set_xlabel('epoch')
    ax1.legend(['train', 'val'], loc='upper left')

    ax2.plot(his.history['loss'])
    ax2.plot(his.history['val_loss'])
    ax2.set_title('model loss')
    ax2.set_ylabel('loss')
    ax2.set_xlabel('epoch')
    ax2.legend(['train', 'val'], loc='upper left')


#Simple fully connected neuronal network architecture 
#parameter arch - number of neurons in each layer, len of arch - number of layers
#numPC - number of principal components 
def simpleNN(x_train, x_test, y_train, y_test, arch, numPC, dropout, dropoutP, flatten, channels):

    architecture = []

    if flatten:
        architecture.append(Flatten(input_shape=(32, 32, channels)))
        for i in range(len(arch)):
            architecture.append(Dense(arch[i], activation='relu'))
            if dropout:
                architecture.append(Dropout(dropoutP))
        architecture.append(Dense(10, activation='softmax'))

        model_SNN = keras.Sequential(architecture)

    else:
        architecture.append(Dense(arch[0], input_dim = numPC, activation='relu'))
        if dropout:
            architecture.append(Dropout(dropoutP))
        for i in range(1, len(arch)):
            architecture.append(Dense(arch[i], activation='relu'))
            if dropout:
                architecture.append(Dropout(dropoutP))
        architecture.append(Dense(10, activation='softmax'))

        model_SNN = keras.Sequential(architecture)

    print(model_SNN.summary())
    model_SNN.compile(optimizer='adam',
                  loss='categorical_crossentropy',
                  metrics=['accuracy'])

    his = model_SNN.fit(x_train, y_train, batch_size=64, epochs=30, 
                            validation_split=0.2)
    
    model_SNN.evaluate(x_test, y_test)
    drawAcc(his)

    return model_SNN


#Convolutional neural network architecture
#paramsFCNN - number of neurons in each layer, len of paramsFCNN - number of layers
#paramsCNN - parameters for CNN [1st layer: number of filters, kernel size for maxpooling layer, stride for maxpooling layer, 2nd layer: ...]
def cnnModels(x_train, x_test, y_train, y_test, paramsCNN, paramsFCNN, NumberOfEpochs, batchSize, optimizer, dropout, dropoutP, channels):
    architecture = []
    activFunc    = keras.layers.LeakyReLU(alpha=0.02) 


    for i in range(0, len(paramsCNN), 3):
        if i == 0: 
            architecture.append(Conv2D(paramsCNN[i], (3,3), padding='same', activation=activFunc, input_shape=(32,32,channels)))
        else:
            architecture.append(Conv2D(paramsCNN[i], (3,3), padding='same', activation=activFunc))
        architecture.append(MaxPooling2D((paramsCNN[i+1], paramsCNN[i+1]), strides=paramsCNN[i+2], padding='valid'))
        if dropout:
           architecture.append(Dropout(dropoutP)) 
    
    architecture.append(Flatten())
    
    for j in range(len(paramsFCNN)):
        architecture.append(Dense(paramsFCNN[j], activation=activFunc))

    architecture.append(Dense(10, activation='softmax'))

    model = keras.Sequential(architecture)
    print(model.summary())

    model.compile(optimizer=optimizer,
                  loss='categorical_crossentropy',
                  metrics=['accuracy'])
    
    his = model.fit(x_train, y_train, batch_size=batchSize, epochs=NumberOfEpochs, 
                validation_split=0.2)
    
    model.evaluate(x_test, y_test)
    drawAcc(his)

    return model

"""**Original data (colour images - 3 channels)**

**Load and preprocess the data**
"""

cifar10 = tf.keras.datasets.cifar10
labels = ['Airplane', 'Automobile', 'Bird', 'Cat', 'Deer', 'Dog', 
          'Frog', 'Horse', 'Ship', 'Truck']
  
x_train, x_test, y_train, y_test, y_test_init = data_preprocessing(cifar10, False)

#show some images in the dataset
show_img(x_train, y_train, labels, False)

"""**Principal component analysis**"""

#Number of principal components, that remain 90% of variance information
check_var_pca(x_train, 0.9, 5, 5)

#Number of principal components, that remain 95% of variance information
check_var_pca(x_train, 0.95, 10, 100)

#first 100 pc (remain 90% of initial variance information)
x_train_reduced, x_test_reduced = myPca(x_train, x_test, 100)
print(x_train_reduced.shape)
print(x_test_reduced.shape)

#first 225 pc (remain 95% of initial variance information)
x_train_reduced_2, x_test_reduced_2 = myPca(x_train, x_test, 225)
print(x_train_reduced_2.shape)
print(x_test_reduced_2.shape)

"""**Experiments with simple fully-connected neuronal network**"""

#Simple NN
#All data without using PCA
arch1 = [1000, 500, 300]
model = simpleNN(x_train, x_test, y_train, y_test, arch1, 0, True, 0.4, True, 3)

#show some missclassified images and confusion matrix
maskFCNN = show_missclassified_img(model, x_test, y_test_init, 20, labels, False, True)



#Simple NN
#All data with PCA (100 PC, 90% of init variance)
arch1 = [1000, 500, 300]
simpleNN(x_train_reduced, x_test_reduced, y_train, y_test, arch1, 100, True, 0.4, False, 3)

#Simple NN
#All data with PCA (225 PC, 95% of init variance)
arch1 = [1000, 500, 300]
simpleNN(x_train_reduced_2, x_test_reduced_2, y_train, y_test, arch1, 225, True, 0.4, False, 3)

#Simple NN
#All data with PCA (100 PC, 90% of init variance)
arch1 = [100, 100, 100]
simpleNN(x_train_reduced, x_test_reduced, y_train, y_test, arch1, 100, True, 0.4, False, 3)

#Simple NN
#All data with PCA (225 PC, 95% of init variance)
arch1 = [100, 100, 100]
simpleNN(x_train_reduced_2, x_test_reduced_2, y_train, y_test, arch1, 225, True, 0.4, False, 3)

#Simple NN
#All data with PCA (100 PC, 90% of init variance)
arch1 = [200, 200, 200]
simpleNN(x_train_reduced, x_test_reduced, y_train, y_test, arch1, 100, True, 0.4, False, 3)

#Simple NN
#All data with PCA (225 PC, 95% of init variance)
arch1 = [200, 200, 200]
simpleNN(x_train_reduced_2, x_test_reduced_2, y_train, y_test, arch1, 225, True, 0.4, False, 3)

#Simple NN
#All data with PCA (100 PC, 90% of init variance)
arch1 = [300, 300, 300]
simpleNN(x_train_reduced, x_test_reduced, y_train, y_test, arch1, 100, True, 0.4, False, 3)

#Simple NN
#All data with PCA (225 PC, 95% of init variance)
arch1 = [300, 300, 300]
simpleNN(x_train_reduced_2, x_test_reduced_2, y_train, y_test, arch1, 225, True, 0.4, False, 3)

#Simple NN
#All data with PCA (225 PC, 95% of init variance)
arch1 = [100, 100]
simpleNN(x_train_reduced_2, x_test_reduced_2, y_train, y_test, arch1, 225, True, 0.4, False, 3)

#Simple NN
#All data with PCA (225 PC, 95% of init variance)
arch1 = [500, 500]
simpleNN(x_train_reduced_2, x_test_reduced_2, y_train, y_test, arch1, 225, True, 0.4, False, 3)

#Simple NN
#All data with PCA (225 PC, 95% of init variance)
arch1 = [200, 200, 200, 100, 100, 100]
simpleNN(x_train_reduced_2, x_test_reduced_2, y_train, y_test, arch1, 225, True, 0.4, False, 3)

"""**Experiments with Convolutional neuronal networks**"""

#params CNN: cnnModels(x_train, x_test, y_train, y_test, paramsCNN, paramsFCNN, NumberOfEpochs, batchSize, optimizer, dropout, dropoutP, channels)
#paramsCNN - (number of filters 1st layer, maxPooling kernel size 1st layer, stride maxPooling 1st layer, ... 2nd layer, 3rd layer, ...)
#paramsFCNN - (number of neurons 1st layer, ... 2nd layer, ...)

"""**CNN architecture with FCNN head without dropout**"""

#CNN model 1
paramsCNN  = [64, 4, 2, 128, 4, 2, 256, 4, 2]
paramsFCNN = [128]

cnnModels(x_train, x_test, y_train, y_test, paramsCNN, paramsFCNN, 25, 64, 'adam', False, 0.25, 3)

"""**CNN architecture without FCNN head without dropout**"""

#CNN model 2
paramsCNN  = [64, 4, 2, 128, 4, 2, 256, 4, 2]
paramsFCNN = []

cnnModels(x_train, x_test, y_train, y_test, paramsCNN, paramsFCNN, 25, 64, 'adam', False, 0.25, 3)

"""**CNN architecture with FCNN head (1 layer) with dropout (0.4)**"""

paramsCNN  = [64, 4, 2, 128, 4, 2, 256, 4, 2]
paramsFCNN = [256]

cnn_1layer = cnnModels(x_train, x_test, y_train, y_test, paramsCNN, paramsFCNN, 25, 64, 'adam', True, 0.4, 3)

#mask for CNN 1 layer
maskCNN_1layer = show_missclassified_img(cnn_1layer, x_test, y_test_init, 20, labels, False, True)

"""**CNN architecture with FCNN head (2 layers) with dropout (0.4)**"""

paramsCNN  = [64, 4, 2, 128, 4, 2, 256, 4, 2]
paramsFCNN = [256, 256]

cnn_2layer2 = cnnModels(x_train, x_test, y_train, y_test, paramsCNN, paramsFCNN, 25, 64, 'adam', True, 0.4, 3)

#show some missclassified images and the confusion matrix of the best CNN model
maskCNN_2layers = show_missclassified_img(cnn_2layer2, x_test, y_test_init, 20, labels, False, True)

#Compare CNN and FCNN (show some images that were missclassified by the simple FCNN
#model, but correctely classified by the CNN)
compareResults(maskCNN_2layers, maskFCNN, y_test_init, labels, 20)

"""**Experiments with grayscaled images (1 channel)**"""

x_train, x_test, y_train, y_test, y_test_init = data_preprocessing(cifar10, True)

show_img(x_train, y_train, labels, True)

#Number of principal components, that remain 90% of variance information
check_var_pca(x_train, 0.9, 5, 5)

#Number of principal components, that remain 95% of variance information
check_var_pca(x_train, 0.95, 10, 100)

#first 165 pc (remain 95% of initial variance information)
x_train_reduced, x_test_reduced = myPca(x_train, x_test, 165)
print(x_train_reduced.shape)
print(x_test_reduced.shape)

#All data with PCA (165 PC, 95% of init variance)
arch1 = [300, 300, 300]
simpleNN(x_train_reduced, x_test_reduced, y_train, y_test, arch1, 165, True, 0.4, False, 1)

paramsCNN   = [64, 4, 2, 128, 4, 2, 256, 4, 2]
paramsFCNN  = [256, 256]

modelCNN_gr = cnnModels(x_train, x_test, y_train, y_test, paramsCNN, paramsFCNN, 25, 64, 'adam', True, 0.4, 1)

show_missclassified_img(modelCNN_gr, x_test, y_test_init, 20, labels, True, True)